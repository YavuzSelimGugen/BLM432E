{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello word.', \"It's good to see you.\"]\n"
     ]
    }
   ],
   "source": [
    "#verilen metini cümlelerine ayırıyor.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "para =\"Hello word. It's good to see you.\"\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'word', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "#metini kelimelerine ayırıyor. noktalama işaretlerini de alıyor.\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'word', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "#metini kelimelerine ve noktalama işaretlerine ayırıyor.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'word', \"It's\", 'good', 'to', 'see', 'you']\n"
     ]
    }
   ],
   "source": [
    "#regular expressions ile her bir kelimeyi almak.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "#boşluklardan ayıran regex. True yerine false olsa bu sefer boşlukları alıyor sadece kelimelerden bölüyor.\n",
    "tokenizer = RegexpTokenizer('\\s+',gaps=True)\n",
    "print(tokenizer.tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "Asian girl: Yeah, being angry!\n",
      "Girl: But you already have a Big Mac...\n",
      "I only have a dollar...Can you spare some change?\n"
     ]
    }
   ],
   "source": [
    "#verilen bir dialoga göre unsupervised eğitiyor ve diyaloglara uygun olarak cevap üretiyor.\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text=webtext.raw('overheard.txt')\n",
    "sent_tokenizer=PunktSentenceTokenizer(text)\n",
    "sents1=sent_tokenizer.tokenize(text)\n",
    "print(sents1[0])\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2=sent_tokenize(text)\n",
    "print(sents2[0])\n",
    "\n",
    "print(sents1[678])\n",
    "print(sents2[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "Girl: But you already have a Big Mac...\n"
     ]
    }
   ],
   "source": [
    "#text dosyasından okumak.\n",
    "with open('/jupiter/overheard.txt',encoding='ISO-8859-2') as f:\n",
    "    text=f.read()\n",
    "sent_tokenizer=PunktSentenceTokenizer(text)\n",
    "sents=sent_tokenizer.tokenize(text)\n",
    "print(sents[0])\n",
    "print(sents[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords, english stopwords'da olmayan stopwordları tespit ediyoruz.\n",
    "from nltk.corpus import stopwords\n",
    "english_stops=set(stopwords.words('english'))\n",
    "words=[\"Can't\",'is','a','contraction']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids() #stopwordleri sistemde kayıtlı olan dilleri görmek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acaba',\n",
       " 'ama',\n",
       " 'aslında',\n",
       " 'az',\n",
       " 'bazı',\n",
       " 'belki',\n",
       " 'biri',\n",
       " 'birkaç',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bu',\n",
       " 'çok',\n",
       " 'çünkü',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'de',\n",
       " 'defa',\n",
       " 'diye',\n",
       " 'eğer',\n",
       " 'en',\n",
       " 'gibi',\n",
       " 'hem',\n",
       " 'hep',\n",
       " 'hepsi',\n",
       " 'her',\n",
       " 'hiç',\n",
       " 'için',\n",
       " 'ile',\n",
       " 'ise',\n",
       " 'kez',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'mı',\n",
       " 'mu',\n",
       " 'mü',\n",
       " 'nasıl',\n",
       " 'ne',\n",
       " 'neden',\n",
       " 'nerde',\n",
       " 'nerede',\n",
       " 'nereye',\n",
       " 'niçin',\n",
       " 'niye',\n",
       " 'o',\n",
       " 'sanki',\n",
       " 'şey',\n",
       " 'siz',\n",
       " 'şu',\n",
       " 'tüm',\n",
       " 've',\n",
       " 'veya',\n",
       " 'ya',\n",
       " 'yani']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('turkish') #bir dildeki stopwordsleri görmek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook.n.01\n",
      "a book of recipes and cooking directions\n",
      "['cooking can be a great art', 'people are needed who have experience in cookery', 'he left the preparation of meals to his wife']\n",
      ">> ** <<\n",
      "[Synset('reference_book.n.01')]\n",
      "[Synset('annual.n.02'), Synset('atlas.n.02'), Synset('cookbook.n.01'), Synset('directory.n.01'), Synset('encyclopedia.n.01'), Synset('handbook.n.01'), Synset('instruction_book.n.01'), Synset('source_book.n.01'), Synset('wordbook.n.01')]\n",
      "[Synset('entity.n.01')]\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('work.n.02'), Synset('publication.n.01'), Synset('book.n.01'), Synset('reference_book.n.01'), Synset('cookbook.n.01')]]\n",
      "kelimenün türü :  n\n"
     ]
    }
   ],
   "source": [
    "#wordnet sözlüğü kelimelerin anlamlarını alabiliyoruz.\n",
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('cookbook')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())\n",
    "print(wordnet.synsets('cooking')[0].examples())\n",
    "#syn=wordnet.synsets('cookbook.n.01') dersek de aynı bilgilere erişebiliyoruz, bu id gibi bir şey.\n",
    "print(\">> ** <<\")\n",
    "#\"A is a hyponym of B\" means that \"A is a type of B\"\n",
    "print(syn.hypernyms()) #referencebook is hyponym of cookbook\n",
    "print(syn.hypernyms()[0].hyponyms())\n",
    "print(syn.root_hypernyms())\n",
    "print(syn.hypernym_paths())#roottan başlar orjinal haline kadar devam eder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kelimenün türü :  n\n",
      "[Synset('great.n.01'), Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.13')]\n",
      "[Synset('great.n.01')]\n",
      "[Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.13')]\n"
     ]
    }
   ],
   "source": [
    "#kelimeler ve türleri\n",
    "print(\"kelimenün türü : \",syn.pos()) #n:noun a:adjective r:adverb v:verb\n",
    "#örn greatın 1 nounu var ve 6 adjectivesi.\n",
    "print(wordnet.synsets('great'))\n",
    "print(wordnet.synsets('great',pos='n'))\n",
    "print(wordnet.synsets('great',pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook\n",
      "cookery_book\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#başlık kelime ve türevleri\n",
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "print(lemmas[0].name())\n",
    "print(lemmas[1].name())\n",
    "print(lemmas[0].synset()==lemmas[1].synset())\n",
    "#[lemma.name() for lemma in syn.lemmas] diyerek hepsini bulabilirsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'book',\n",
       " 'volume',\n",
       " 'record',\n",
       " 'record_book',\n",
       " 'book',\n",
       " 'script',\n",
       " 'book',\n",
       " 'playscript',\n",
       " 'ledger',\n",
       " 'leger',\n",
       " 'account_book',\n",
       " 'book_of_account',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'rule_book',\n",
       " 'Koran',\n",
       " 'Quran',\n",
       " \"al-Qur'an\",\n",
       " 'Book',\n",
       " 'Bible',\n",
       " 'Christian_Bible',\n",
       " 'Book',\n",
       " 'Good_Book',\n",
       " 'Holy_Scripture',\n",
       " 'Holy_Writ',\n",
       " 'Scripture',\n",
       " 'Word_of_God',\n",
       " 'Word',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'reserve',\n",
       " 'hold',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#synonyms : mümkün olabilecek tüm eş anlamlıları bul.\n",
    "synonyms=[]\n",
    "for syn in wordnet.synsets('book'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moral excellence or admirableness\n",
      "evil\n",
      "the quality of being morally wrong in principle or practice\n",
      "having desirable or positive qualities especially those suitable for a thing specified\n",
      "bad\n",
      "having undesirable or negative qualities\n"
     ]
    }
   ],
   "source": [
    "#antonyms : zıt anlamlılarını bul\n",
    "gn2=wordnet.synset('good.n.02')\n",
    "print(gn2.definition())\n",
    "evil=gn2.lemmas()[0].antonyms()[0]\n",
    "print(evil.name())\n",
    "print(evil.synset().definition())\n",
    "ga1=wordnet.synset('good.a.01')\n",
    "print(ga1.definition())\n",
    "bad=ga1.lemmas()[0].antonyms()[0]\n",
    "print(bad.name())\n",
    "print(bad.synset().definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarity\n",
    "#önceki örneklerde instructionbooku cookbookun  hyponymi olduğunu bulmuştuk. Kelime olarak biraz benzeseler de anlam olarak\n",
    "#ne kadar benzediklerini bulabiliriz.\n",
    "from nltk.corpus import wordnet\n",
    "cb=wordnet.synset('cookbook.n.01')\n",
    "ib=wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#birbirine benzemeyen kelimeleri karşılaştıralım(anlam bakımından)\n",
    "dog=wordnet.synsets('dog')[0]\n",
    "dog.wup_similarity(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verb karşılaştırmaları bebzerlikleri\n",
    "cook=wordnet.synset('cook.v.01')\n",
    "bake=wordnet.synset('bake.v.02')\n",
    "cook.wup_similarity(bake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collocations, sıklıkla birlikte kullanılan iki yada daha fazla kelime.\n",
    "#burada bir yazı dosyasındaki sıklıkla birlikte kullanılan kelimeleri arıyoruz.\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "words=[w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf=BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yukarıdaki örneğin daha iyi hali noktalama işaretleri vb kötü ayrıntılar olmadan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopset=set(stopwords.words('english'))\n",
    "filter_stops=lambda w:len(w) <3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio,4)#kaç set görmek istiyorsak onun numarası veriliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 'term', 'relationship')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yukarıdakilerde 2li kelimelere bakılmıştı. şimdi 3lü sık kullanılan kelimelere bakalım.\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "words=[w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf=TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "tcf.apply_freq_filter(3) #2li en çok kullanlanlar için 2 yazarız.\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
